{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICA Dark Matter — Automated Interpretability\n",
    "\n",
    "Run autointerp on all 100 ICA components + 100 baseline SAE features.\n",
    "Uses Claude Sonnet for explain + score passes (Bills et al. methodology).\n",
    "\n",
    "**Run this on the RunPod instance where `outputs/` already exists.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Load Data\n",
    "# !pip install anthropic sae-lens transformer-lens\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import kurtosis\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import re\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "AUTOINTERP_DIR = \"autointerp_results\"\n",
    "os.makedirs(AUTOINTERP_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Load saved arrays\n",
    "print(\"Loading saved arrays...\")\n",
    "activations = np.load(os.path.join(OUTPUT_DIR, \"activations.npy\"))\n",
    "tokens_flat = np.load(os.path.join(OUTPUT_DIR, \"tokens.npy\"))\n",
    "ica_directions = np.load(os.path.join(OUTPUT_DIR, \"ica_directions.npy\"))\n",
    "ica_activations = np.load(os.path.join(OUTPUT_DIR, \"ica_activations.npy\"))\n",
    "\n",
    "n_ica_components = ica_directions.shape[0]\n",
    "n_tokens = activations.shape[0]\n",
    "\n",
    "print(f\"  activations: {activations.shape}\")\n",
    "print(f\"  tokens: {tokens_flat.shape}\")\n",
    "print(f\"  ica_directions: {ica_directions.shape}\")\n",
    "print(f\"  ica_activations: {ica_activations.shape}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Model (for tokenizer) and SAE\n",
    "from sae_lens import SAE\n",
    "\n",
    "try:\n",
    "    from sae_lens import HookedSAETransformer\n",
    "    model = HookedSAETransformer.from_pretrained(\"gpt2\", device=DEVICE)\n",
    "except Exception:\n",
    "    from transformer_lens import HookedTransformer\n",
    "    model = HookedTransformer.from_pretrained(\"gpt2\", device=DEVICE)\n",
    "\n",
    "sae = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.6.hook_resid_pre\",\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "print(\"Model and SAE loaded.\")\n",
    "\n",
    "# Compute ICA-SAE cosine similarities (needed to identify novel components)\n",
    "sae_decoder = sae.W_dec.detach().cpu().float().numpy()\n",
    "sae_decoder_norms = np.maximum(np.linalg.norm(sae_decoder, axis=1, keepdims=True), 1e-8)\n",
    "sae_decoder_normed = sae_decoder / sae_decoder_norms\n",
    "cos_sim = ica_directions @ sae_decoder_normed.T\n",
    "max_cos_sim = np.max(np.abs(cos_sim), axis=1)\n",
    "del cos_sim\n",
    "\n",
    "n_novel = (max_cos_sim < 0.3).sum()\n",
    "print(f\"Novel ICA components (cosine < 0.3): {n_novel}\")\n",
    "print(f\"Mean max cosine sim: {max_cos_sim.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Set your API key\n",
    "import anthropic\n",
    "\n",
    "# Set your API key — either uncomment and paste, or set env var\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"\n",
    "\n",
    "client = anthropic.Anthropic()  # reads ANTHROPIC_API_KEY from env\n",
    "print(\"Anthropic client initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Collect Rich Activation Examples\n\nCONTEXT_BEFORE = 50  # tokens before highlighted token\nCONTEXT_AFTER = 20   # tokens after highlighted token\nTOP_K = 40           # collect more to survive deduplication (need 20 usable)\n\n\ndef get_context(idx, tokens, tokenizer, ctx_before=CONTEXT_BEFORE, ctx_after=CONTEXT_AFTER):\n    \"\"\"Get surrounding context for a token at position idx.\"\"\"\n    start = max(0, idx - ctx_before)\n    end = min(len(tokens), idx + ctx_after + 1)\n\n    before_ids = tokens[start:idx].tolist()\n    target_id = int(tokens[idx])\n    after_ids = tokens[idx + 1:end].tolist()\n\n    before_text = tokenizer.decode(before_ids)\n    target_text = tokenizer.decode([target_id])\n    after_text = tokenizer.decode(after_ids)\n\n    highlighted = f\"{before_text}[{target_text}]{after_text}\"\n    return highlighted, target_text\n\n\ndef collect_examples_for_activations(act_values, tokens, tokenizer, top_k=TOP_K, min_examples=20):\n    \"\"\"Collect top activating examples with context, with deduplication.\"\"\"\n    # Get more candidates than needed to survive deduplication\n    n_candidates = min(top_k * 2, len(act_values))\n    top_indices = np.argsort(np.abs(act_values))[-n_candidates:][::-1]\n\n    examples = []\n    seen_contexts = set()\n    for idx in top_indices:\n        if len(examples) >= top_k:\n            break\n        context, token_text = get_context(idx, tokens, tokenizer)\n        # Deduplicate near-identical contexts\n        context_key = context[:80]\n        if context_key in seen_contexts:\n            continue\n        seen_contexts.add(context_key)\n        examples.append({\n            \"context\": context,\n            \"token\": token_text,\n            \"activation\": float(act_values[idx]),\n            \"position\": int(idx),\n        })\n    return examples\n\n\ndef collect_negative_examples(act_values, tokens, tokenizer, n=10):\n    \"\"\"Collect random non-activating examples.\"\"\"\n    # For sparse features (like SAE), most activations are 0.\n    # Use tokens where the feature does NOT fire (activation == 0 or very low).\n    abs_acts = np.abs(act_values)\n\n    # Use the 25th percentile as threshold — works for both sparse and dense activations\n    threshold = np.percentile(abs_acts, 25)\n    # Ensure threshold is at least slightly above 0 for sparse features\n    if threshold <= 0:\n        # Feature is sparse — just pick from the zeros\n        low_indices = np.where(abs_acts == 0)[0]\n        if len(low_indices) == 0:\n            # Everything is nonzero — use bottom 25%\n            low_indices = np.argsort(abs_acts)[:len(abs_acts) // 4]\n    else:\n        low_indices = np.where(abs_acts <= threshold)[0]\n\n    if len(low_indices) == 0:\n        return []\n\n    chosen = np.random.choice(low_indices, size=min(n, len(low_indices)), replace=False)\n\n    examples = []\n    for idx in chosen:\n        context, token_text = get_context(idx, tokens, tokenizer)\n        examples.append({\n            \"context\": context,\n            \"token\": token_text,\n            \"activation\": float(act_values[idx]),\n            \"position\": int(idx),\n        })\n    return examples\n\n\n# --- Collect ICA component examples ---\nprint(f\"Collecting examples for {n_ica_components} ICA components...\")\nica_examples = {}\nfor i in range(n_ica_components):\n    acts = ica_activations[:, i]\n    positives = collect_examples_for_activations(acts, tokens_flat, tokenizer)\n    negatives = collect_negative_examples(acts, tokens_flat, tokenizer, n=10)\n    ica_examples[i] = {\"positives\": positives, \"negatives\": negatives}\n    if (i + 1) % 20 == 0:\n        print(f\"  {i+1}/{n_ica_components} ICA components done \"\n              f\"(last: {len(positives)} pos, {len(negatives)} neg)\")\n\n# --- Select and collect SAE baseline features ---\nprint(f\"\\nSelecting 100 baseline SAE features...\")\n\n# Compute SAE feature activation density in chunks\nchunk_size = 50_000\nn_chunks = (n_tokens + chunk_size - 1) // chunk_size\nn_sae_features = sae.W_dec.shape[0]\nfire_counts = np.zeros(n_sae_features, dtype=np.int64)\n\nfor c in range(n_chunks):\n    s, e = c * chunk_size, min((c + 1) * chunk_size, n_tokens)\n    chunk = torch.tensor(activations[s:e], device=DEVICE, dtype=torch.float32)\n    with torch.no_grad():\n        sae_acts = sae.encode(chunk)\n        fire_counts += (sae_acts > 0).sum(dim=0).cpu().numpy()\n    del sae_acts, chunk\n\nfire_rate = fire_counts / n_tokens\n# Pick 100 active SAE features (fire on >0.1% of tokens), randomly sampled\nactive_sae_indices = np.where(fire_rate > 0.001)[0]\nbaseline_sae_indices = np.random.choice(active_sae_indices, size=min(100, len(active_sae_indices)), replace=False)\nbaseline_sae_indices.sort()\nprint(f\"  Selected {len(baseline_sae_indices)} SAE features from {len(active_sae_indices)} active features\")\n\n# Collect SAE feature activations for baseline features\nprint(f\"Collecting SAE feature activations for baseline...\")\nsae_feature_acts = np.zeros((n_tokens, len(baseline_sae_indices)), dtype=np.float32)\nfor c in range(n_chunks):\n    s, e = c * chunk_size, min((c + 1) * chunk_size, n_tokens)\n    chunk = torch.tensor(activations[s:e], device=DEVICE, dtype=torch.float32)\n    with torch.no_grad():\n        sae_acts = sae.encode(chunk)\n        sae_feature_acts[s:e] = sae_acts[:, baseline_sae_indices].cpu().numpy()\n    del sae_acts, chunk\n\nif DEVICE == \"cuda\":\n    torch.cuda.empty_cache()\n\n# Collect examples for SAE features\nprint(f\"Collecting examples for {len(baseline_sae_indices)} SAE features...\")\nsae_examples = {}\nfor j, feat_idx in enumerate(baseline_sae_indices):\n    acts = sae_feature_acts[:, j]\n    positives = collect_examples_for_activations(acts, tokens_flat, tokenizer)\n    negatives = collect_negative_examples(acts, tokens_flat, tokenizer, n=10)\n    sae_examples[int(feat_idx)] = {\"positives\": positives, \"negatives\": negatives}\n    if (j + 1) % 20 == 0:\n        print(f\"  {j+1}/{len(baseline_sae_indices)} SAE features done \"\n              f\"(last: {len(positives)} pos, {len(negatives)} neg)\")\n\ndel sae_feature_acts\n\n# Save examples\nwith open(os.path.join(AUTOINTERP_DIR, \"ica_examples.json\"), \"w\") as f:\n    json.dump({str(k): v for k, v in ica_examples.items()}, f)\nwith open(os.path.join(AUTOINTERP_DIR, \"sae_examples.json\"), \"w\") as f:\n    json.dump({str(k): v for k, v in sae_examples.items()}, f)\nwith open(os.path.join(AUTOINTERP_DIR, \"baseline_sae_indices.json\"), \"w\") as f:\n    json.dump(baseline_sae_indices.tolist(), f)\n\nprint(f\"\\nSaved ica_examples.json and sae_examples.json to {AUTOINTERP_DIR}/\")\nprint(f\"Total ICA positives: {sum(len(v['positives']) for v in ica_examples.values())}\")\nprint(f\"Total ICA negatives: {sum(len(v['negatives']) for v in ica_examples.values())}\")\nprint(f\"Total SAE positives: {sum(len(v['positives']) for v in sae_examples.values())}\")\nprint(f\"Total SAE negatives: {sum(len(v['negatives']) for v in sae_examples.values())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Explain Pass — Generate interpretations via Claude\n",
    "\n",
    "EXPLAIN_MODEL = \"claude-sonnet-4-20250514\"\n",
    "N_EXPLAIN_EXAMPLES = 10  # Use top 10 for explanation, reserve rest for scoring\n",
    "\n",
    "EXPLAIN_SYSTEM = \"\"\"You are an interpretability researcher analyzing directions in a neural network's activation space. You will be shown text excerpts where a particular direction activates strongly. The token in [brackets] is where the direction fires. Your job is to identify the common pattern, concept, or feature these examples share.\n",
    "\n",
    "Respond with:\n",
    "1. A short label (2-5 words) for what this direction represents\n",
    "2. A one-sentence explanation\n",
    "3. A confidence score from 1-5:\n",
    "   - 5: Clear, monosemantic concept\n",
    "   - 4: Coherent theme with minor variation\n",
    "   - 3: Plausible pattern but somewhat noisy\n",
    "   - 2: Weak or ambiguous pattern\n",
    "   - 1: No discernible pattern / random\n",
    "\n",
    "Respond ONLY with valid JSON, no other text:\n",
    "{\"label\": \"...\", \"explanation\": \"...\", \"confidence\": N}\"\"\"\n",
    "\n",
    "\n",
    "def build_explain_prompt(examples, component_id, component_type=\"direction\"):\n",
    "    \"\"\"Build the user prompt for the explain pass.\"\"\"\n",
    "    lines = [f\"Here are the top {len(examples)} activating examples for {component_type} #{component_id}:\\n\"]\n",
    "    for i, ex in enumerate(examples):\n",
    "        lines.append(f\"Example {i+1} (activation: {ex['activation']:.3f}):\")\n",
    "        lines.append(ex[\"context\"])\n",
    "        lines.append(\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def call_explain(examples, component_id, component_type=\"direction\"):\n",
    "    \"\"\"Call Claude to explain a component.\"\"\"\n",
    "    prompt = build_explain_prompt(examples[:N_EXPLAIN_EXAMPLES], component_id, component_type)\n",
    "    response = client.messages.create(\n",
    "        model=EXPLAIN_MODEL,\n",
    "        max_tokens=256,\n",
    "        temperature=0,\n",
    "        system=EXPLAIN_SYSTEM,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    text = response.content[0].text.strip()\n",
    "    # Parse JSON — handle potential markdown wrapping\n",
    "    text = re.sub(r'^```json\\s*', '', text)\n",
    "    text = re.sub(r'\\s*```$', '', text)\n",
    "    try:\n",
    "        result = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        result = {\"label\": \"PARSE_ERROR\", \"explanation\": text, \"confidence\": 0}\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- Run explain pass on ICA components ---\n",
    "explanations = {\"ica\": {}, \"sae\": {}}\n",
    "\n",
    "print(f\"Running explain pass on {n_ica_components} ICA components...\")\n",
    "for i in range(n_ica_components):\n",
    "    positives = ica_examples[i][\"positives\"]\n",
    "    if len(positives) < 3:\n",
    "        explanations[\"ica\"][str(i)] = {\"label\": \"TOO_FEW_EXAMPLES\", \"explanation\": \"\", \"confidence\": 0}\n",
    "        continue\n",
    "    result = call_explain(positives, i, \"ICA component\")\n",
    "    result[\"max_cos_sim\"] = float(max_cos_sim[i])\n",
    "    explanations[\"ica\"][str(i)] = result\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  {i+1}/{n_ica_components} — last: [{result.get('confidence', '?')}] {result.get('label', '?')}\")\n",
    "    time.sleep(0.5)  # Rate limit\n",
    "\n",
    "# --- Run explain pass on SAE features ---\n",
    "print(f\"\\nRunning explain pass on {len(baseline_sae_indices)} SAE features...\")\n",
    "for j, feat_idx in enumerate(baseline_sae_indices):\n",
    "    positives = sae_examples[int(feat_idx)][\"positives\"]\n",
    "    if len(positives) < 3:\n",
    "        explanations[\"sae\"][str(feat_idx)] = {\"label\": \"TOO_FEW_EXAMPLES\", \"explanation\": \"\", \"confidence\": 0}\n",
    "        continue\n",
    "    result = call_explain(positives, int(feat_idx), \"SAE feature\")\n",
    "    explanations[\"sae\"][str(feat_idx)] = result\n",
    "    if (j + 1) % 10 == 0:\n",
    "        print(f\"  {j+1}/{len(baseline_sae_indices)} — last: [{result.get('confidence', '?')}] {result.get('label', '?')}\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Save explanations\n",
    "with open(os.path.join(AUTOINTERP_DIR, \"explanations.json\"), \"w\") as f:\n",
    "    json.dump(explanations, f, indent=2)\n",
    "print(f\"\\nSaved explanations.json\")\n",
    "\n",
    "# Quick summary\n",
    "ica_confs = [v[\"confidence\"] for v in explanations[\"ica\"].values() if v[\"confidence\"] > 0]\n",
    "sae_confs = [v[\"confidence\"] for v in explanations[\"sae\"].values() if v[\"confidence\"] > 0]\n",
    "print(f\"ICA mean confidence: {np.mean(ica_confs):.2f} (n={len(ica_confs)})\")\n",
    "print(f\"SAE mean confidence: {np.mean(sae_confs):.2f} (n={len(sae_confs)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Score Pass — Detection accuracy via Claude\n\nSCORE_MODEL = \"claude-sonnet-4-20250514\"\nN_EXPLAIN_EXAMPLES = 10  # first 10 used for explain, rest for scoring\n\nSCORE_SYSTEM = \"\"\"You are evaluating whether a proposed explanation for a neural network direction is correct. You will be given an explanation and a set of text excerpts. For each excerpt, predict whether this direction would activate strongly on the [bracketed] token (YES or NO).\n\nRespond ONLY with a valid JSON array of predictions, no other text:\n[{\"example_id\": 1, \"prediction\": \"YES\"}, {\"example_id\": 2, \"prediction\": \"NO\"}, ...]\"\"\"\n\n\ndef build_score_prompt(explanation, examples_with_labels):\n    \"\"\"Build the scoring prompt with shuffled positive + negative examples.\"\"\"\n    lines = [f'The proposed explanation is: \"{explanation}\"\\n']\n    for i, (ex, _) in enumerate(examples_with_labels):\n        lines.append(f\"Example {i+1}:\")\n        lines.append(ex[\"context\"])\n        lines.append(\"\")\n    return \"\\n\".join(lines)\n\n\ndef call_score(explanation, positives, negatives):\n    \"\"\"Call Claude to score an explanation against held-out examples.\"\"\"\n    # Use held-out positives (after the first N_EXPLAIN_EXAMPLES) + negatives\n    held_out_pos = positives[N_EXPLAIN_EXAMPLES:]\n    neg = negatives\n\n    # Lower threshold to 3 to handle sparse features with fewer examples\n    if len(held_out_pos) < 3 or len(neg) < 3:\n        return {\n            \"balanced_accuracy\": None,\n            \"reason\": f\"too_few_examples (pos={len(held_out_pos)}, neg={len(neg)})\",\n        }\n\n    # Combine and shuffle\n    examples_with_labels = [(ex, True) for ex in held_out_pos] + [(ex, False) for ex in neg]\n    np.random.shuffle(examples_with_labels)\n    ground_truth = [label for _, label in examples_with_labels]\n    n_total = len(examples_with_labels)\n\n    prompt = build_score_prompt(explanation, examples_with_labels)\n    response = client.messages.create(\n        model=SCORE_MODEL,\n        max_tokens=1024,\n        temperature=0,\n        system=SCORE_SYSTEM.replace(\"20 predictions\", f\"{n_total} predictions\"),\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n    text = response.content[0].text.strip()\n    text = re.sub(r'^```json\\s*', '', text)\n    text = re.sub(r'\\s*```$', '', text)\n\n    try:\n        predictions = json.loads(text)\n        pred_yes = [p[\"prediction\"].upper() == \"YES\" for p in predictions]\n    except (json.JSONDecodeError, KeyError, TypeError):\n        return {\"balanced_accuracy\": None, \"reason\": \"parse_error\", \"raw\": text[:200]}\n\n    if len(pred_yes) != len(ground_truth):\n        return {\n            \"balanced_accuracy\": None,\n            \"reason\": f\"length_mismatch ({len(pred_yes)} vs {len(ground_truth)})\",\n        }\n\n    # Compute balanced accuracy\n    tp = sum(1 for p, g in zip(pred_yes, ground_truth) if p and g)\n    tn = sum(1 for p, g in zip(pred_yes, ground_truth) if not p and not g)\n    n_pos = sum(ground_truth)\n    n_neg = len(ground_truth) - n_pos\n    tpr = tp / n_pos if n_pos > 0 else 0\n    tnr = tn / n_neg if n_neg > 0 else 0\n    balanced_acc = (tpr + tnr) / 2\n\n    return {\n        \"balanced_accuracy\": float(balanced_acc),\n        \"tpr\": float(tpr),\n        \"tnr\": float(tnr),\n        \"n_pos\": n_pos,\n        \"n_neg\": n_neg,\n    }\n\n\n# --- Run score pass on ICA components ---\nscores = {\"ica\": {}, \"sae\": {}}\n\nprint(f\"Running score pass on {n_ica_components} ICA components...\")\nfor i in range(n_ica_components):\n    exp = explanations[\"ica\"].get(str(i), {})\n    if exp.get(\"confidence\", 0) == 0:\n        scores[\"ica\"][str(i)] = {\"balanced_accuracy\": None, \"reason\": \"skipped_low_confidence\"}\n        continue\n    result = call_score(\n        exp[\"explanation\"],\n        ica_examples[i][\"positives\"],\n        ica_examples[i][\"negatives\"],\n    )\n    scores[\"ica\"][str(i)] = result\n    if (i + 1) % 10 == 0:\n        acc = result.get('balanced_accuracy', '?')\n        acc_str = f\"{acc:.2f}\" if isinstance(acc, float) else str(acc)\n        print(f\"  {i+1}/{n_ica_components} — last acc: {acc_str}\")\n    time.sleep(0.5)\n\n# --- Run score pass on SAE features ---\nprint(f\"\\nRunning score pass on {len(baseline_sae_indices)} SAE features...\")\nn_sae_skipped = 0\nfor j, feat_idx in enumerate(baseline_sae_indices):\n    key = str(int(feat_idx))\n    exp = explanations[\"sae\"].get(key, {})\n    if exp.get(\"confidence\", 0) == 0:\n        scores[\"sae\"][key] = {\"balanced_accuracy\": None, \"reason\": \"skipped_low_confidence\"}\n        n_sae_skipped += 1\n        continue\n    result = call_score(\n        exp[\"explanation\"],\n        sae_examples[int(feat_idx)][\"positives\"],\n        sae_examples[int(feat_idx)][\"negatives\"],\n    )\n    scores[\"sae\"][key] = result\n    if (j + 1) % 10 == 0:\n        acc = result.get('balanced_accuracy')\n        reason = result.get('reason', '')\n        if acc is not None:\n            print(f\"  {j+1}/{len(baseline_sae_indices)} — last acc: {acc:.2f}\")\n        else:\n            print(f\"  {j+1}/{len(baseline_sae_indices)} — last: {reason}\")\n    time.sleep(0.5)\n\n# Save scores\nwith open(os.path.join(AUTOINTERP_DIR, \"scores.json\"), \"w\") as f:\n    json.dump(scores, f, indent=2)\n\n# Diagnostic summary\nica_scored = sum(1 for v in scores[\"ica\"].values() if v.get(\"balanced_accuracy\") is not None)\nsae_scored = sum(1 for v in scores[\"sae\"].values() if v.get(\"balanced_accuracy\") is not None)\nsae_reasons = defaultdict(int)\nfor v in scores[\"sae\"].values():\n    if v.get(\"balanced_accuracy\") is None:\n        sae_reasons[v.get(\"reason\", \"unknown\")] += 1\n\nprint(f\"\\nScoring complete:\")\nprint(f\"  ICA: {ica_scored}/{n_ica_components} scored successfully\")\nprint(f\"  SAE: {sae_scored}/{len(baseline_sae_indices)} scored successfully\")\nif sae_reasons:\n    print(f\"  SAE failures:\")\n    for reason, count in sae_reasons.items():\n        print(f\"    {reason}: {count}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Aggregate Results & Generate Tables\n",
    "\n",
    "# Load if needed\n",
    "if 'explanations' not in dir():\n",
    "    with open(os.path.join(AUTOINTERP_DIR, \"explanations.json\")) as f:\n",
    "        explanations = json.load(f)\n",
    "    with open(os.path.join(AUTOINTERP_DIR, \"scores.json\")) as f:\n",
    "        scores = json.load(f)\n",
    "\n",
    "# --- Gather ICA stats ---\n",
    "ica_confs = []\n",
    "ica_accs = []\n",
    "ica_novel_confs = []\n",
    "ica_novel_accs = []\n",
    "ica_overlap_confs = []\n",
    "ica_overlap_accs = []\n",
    "\n",
    "for i in range(n_ica_components):\n",
    "    exp = explanations[\"ica\"].get(str(i), {})\n",
    "    sc = scores[\"ica\"].get(str(i), {})\n",
    "    conf = exp.get(\"confidence\", 0)\n",
    "    acc = sc.get(\"balanced_accuracy\")\n",
    "\n",
    "    if conf > 0:\n",
    "        ica_confs.append(conf)\n",
    "    if acc is not None:\n",
    "        ica_accs.append(acc)\n",
    "\n",
    "    is_novel = max_cos_sim[i] < 0.3\n",
    "    if is_novel:\n",
    "        if conf > 0:\n",
    "            ica_novel_confs.append(conf)\n",
    "        if acc is not None:\n",
    "            ica_novel_accs.append(acc)\n",
    "    else:\n",
    "        if conf > 0:\n",
    "            ica_overlap_confs.append(conf)\n",
    "        if acc is not None:\n",
    "            ica_overlap_accs.append(acc)\n",
    "\n",
    "# --- Gather SAE stats ---\n",
    "sae_confs = []\n",
    "sae_accs = []\n",
    "for feat_idx in baseline_sae_indices:\n",
    "    exp = explanations[\"sae\"].get(str(feat_idx), {})\n",
    "    sc = scores[\"sae\"].get(str(feat_idx), {})\n",
    "    conf = exp.get(\"confidence\", 0)\n",
    "    acc = sc.get(\"balanced_accuracy\")\n",
    "    if conf > 0:\n",
    "        sae_confs.append(conf)\n",
    "    if acc is not None:\n",
    "        sae_accs.append(acc)\n",
    "\n",
    "\n",
    "def safe_mean(lst):\n",
    "    return np.mean(lst) if lst else float('nan')\n",
    "\n",
    "def safe_median(lst):\n",
    "    return np.median(lst) if lst else float('nan')\n",
    "\n",
    "def frac_above(lst, threshold):\n",
    "    return np.mean([x >= threshold for x in lst]) if lst else float('nan')\n",
    "\n",
    "\n",
    "# --- Print Table 1 ---\n",
    "print(\"=\" * 70)\n",
    "print(\"TABLE 1: Interpretability Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<40s} {'ICA (n=100)':>14s} {'SAE (n=100)':>14s}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Mean confidence (1-5)':<40s} {safe_mean(ica_confs):>14.2f} {safe_mean(sae_confs):>14.2f}\")\n",
    "print(f\"{'Median confidence':<40s} {safe_median(ica_confs):>14.2f} {safe_median(sae_confs):>14.2f}\")\n",
    "print(f\"{'Detection balanced acc (mean)':<40s} {safe_mean(ica_accs):>14.3f} {safe_mean(sae_accs):>14.3f}\")\n",
    "print(f\"{'Detection balanced acc (median)':<40s} {safe_median(ica_accs):>14.3f} {safe_median(sae_accs):>14.3f}\")\n",
    "print(f\"{'Fraction confidence >= 3':<40s} {frac_above(ica_confs, 3):>14.2f} {frac_above(sae_confs, 3):>14.2f}\")\n",
    "print(f\"{'Fraction detection acc > 0.7':<40s} {frac_above(ica_accs, 0.7):>14.2f} {frac_above(sae_accs, 0.7):>14.2f}\")\n",
    "print()\n",
    "\n",
    "# --- Print Table 2 ---\n",
    "print(\"=\" * 70)\n",
    "print(\"TABLE 2: ICA Breakdown by Novelty\")\n",
    "print(\"=\" * 70)\n",
    "n_novel_total = (max_cos_sim < 0.3).sum()\n",
    "n_overlap_total = (max_cos_sim >= 0.3).sum()\n",
    "print(f\"{'Metric':<40s} {'Novel (<0.3)':>14s} {'Overlapping':>14s}\")\n",
    "print(f\"{'':40s} {'n='+str(n_novel_total):>14s} {'n='+str(n_overlap_total):>14s}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Mean detection accuracy':<40s} {safe_mean(ica_novel_accs):>14.3f} {safe_mean(ica_overlap_accs):>14.3f}\")\n",
    "print(f\"{'Mean confidence':<40s} {safe_mean(ica_novel_confs):>14.2f} {safe_mean(ica_overlap_confs):>14.2f}\")\n",
    "print(f\"{'Fraction acc > 0.7':<40s} {frac_above(ica_novel_accs, 0.7):>14.2f} {frac_above(ica_overlap_accs, 0.7):>14.2f}\")\n",
    "print()\n",
    "\n",
    "# --- Interpretation ---\n",
    "print(\"=\" * 70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 70)\n",
    "novel_mean = safe_mean(ica_novel_accs)\n",
    "if not np.isnan(novel_mean):\n",
    "    if novel_mean > 0.7:\n",
    "        print(f\"Novel ICA components have mean detection acc = {novel_mean:.3f} (> 0.7)\")\n",
    "        print(\"=> Strong evidence: dark matter contains interpretable features comparable to SAE features!\")\n",
    "    elif novel_mean > 0.6:\n",
    "        print(f\"Novel ICA components have mean detection acc = {novel_mean:.3f} (> 0.6)\")\n",
    "        print(\"=> Good evidence: dark matter has interpretable structure, somewhat noisier than SAE features.\")\n",
    "    else:\n",
    "        print(f\"Novel ICA components have mean detection acc = {novel_mean:.3f} (near chance)\")\n",
    "        print(\"=> Weak/null result: novel ICA components may not be robustly interpretable.\")\n",
    "else:\n",
    "    print(\"Could not compute novel ICA detection accuracy (insufficient data).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Qualitative Showcase — Top Novel ICA Components\n",
    "\n",
    "# Find novel ICA components with highest detection accuracy\n",
    "novel_results = []\n",
    "for i in range(n_ica_components):\n",
    "    if max_cos_sim[i] >= 0.3:\n",
    "        continue\n",
    "    exp = explanations[\"ica\"].get(str(i), {})\n",
    "    sc = scores[\"ica\"].get(str(i), {})\n",
    "    acc = sc.get(\"balanced_accuracy\")\n",
    "    if acc is None:\n",
    "        continue\n",
    "    novel_results.append({\n",
    "        \"component\": i,\n",
    "        \"label\": exp.get(\"label\", \"?\"),\n",
    "        \"explanation\": exp.get(\"explanation\", \"?\"),\n",
    "        \"confidence\": exp.get(\"confidence\", 0),\n",
    "        \"detection_acc\": acc,\n",
    "        \"max_cos_sim\": float(max_cos_sim[i]),\n",
    "    })\n",
    "\n",
    "novel_results.sort(key=lambda x: x[\"detection_acc\"], reverse=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SHOWCASE: Top Novel ICA Components (cosine < 0.3 to any SAE feature)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rank, r in enumerate(novel_results[:10]):\n",
    "    comp = r[\"component\"]\n",
    "    print(f\"\\n--- #{rank+1}: ICA Component {comp} ---\")\n",
    "    print(f\"  Label:          {r['label']}\")\n",
    "    print(f\"  Explanation:    {r['explanation']}\")\n",
    "    print(f\"  Confidence:     {r['confidence']}/5\")\n",
    "    print(f\"  Detection Acc:  {r['detection_acc']:.3f}\")\n",
    "    print(f\"  SAE Cosine Sim: {r['max_cos_sim']:.3f}\")\n",
    "    print(f\"  Top 3 activating examples:\")\n",
    "    positives = ica_examples[comp][\"positives\"]\n",
    "    for j, ex in enumerate(positives[:3]):\n",
    "        ctx = ex['context']\n",
    "        if len(ctx) > 150:\n",
    "            # Trim to center around the bracketed token\n",
    "            bracket_pos = ctx.find('[')\n",
    "            if bracket_pos > 0:\n",
    "                start = max(0, bracket_pos - 60)\n",
    "                end = min(len(ctx), bracket_pos + 90)\n",
    "                ctx = '...' + ctx[start:end] + '...'\n",
    "        print(f\"    {j+1}. [{ex['activation']:+.2f}] {ctx}\")\n",
    "\n",
    "# Save showcase\n",
    "showcase = {\"novel_top_components\": novel_results[:10]}\n",
    "with open(os.path.join(AUTOINTERP_DIR, \"showcase.json\"), \"w\") as f:\n",
    "    json.dump(showcase, f, indent=2)\n",
    "\n",
    "# Save full results summary\n",
    "summary = {\n",
    "    \"table1\": {\n",
    "        \"ica\": {\n",
    "            \"mean_confidence\": float(safe_mean(ica_confs)),\n",
    "            \"median_confidence\": float(safe_median(ica_confs)),\n",
    "            \"mean_detection_acc\": float(safe_mean(ica_accs)),\n",
    "            \"frac_confidence_gte_3\": float(frac_above(ica_confs, 3)),\n",
    "            \"frac_acc_gt_0.7\": float(frac_above(ica_accs, 0.7)),\n",
    "        },\n",
    "        \"sae\": {\n",
    "            \"mean_confidence\": float(safe_mean(sae_confs)),\n",
    "            \"median_confidence\": float(safe_median(sae_confs)),\n",
    "            \"mean_detection_acc\": float(safe_mean(sae_accs)),\n",
    "            \"frac_confidence_gte_3\": float(frac_above(sae_confs, 3)),\n",
    "            \"frac_acc_gt_0.7\": float(frac_above(sae_accs, 0.7)),\n",
    "        },\n",
    "    },\n",
    "    \"table2\": {\n",
    "        \"novel\": {\n",
    "            \"n\": int(n_novel_total),\n",
    "            \"mean_detection_acc\": float(safe_mean(ica_novel_accs)),\n",
    "            \"mean_confidence\": float(safe_mean(ica_novel_confs)),\n",
    "        },\n",
    "        \"overlapping\": {\n",
    "            \"n\": int(n_overlap_total),\n",
    "            \"mean_detection_acc\": float(safe_mean(ica_overlap_accs)),\n",
    "            \"mean_confidence\": float(safe_mean(ica_overlap_confs)),\n",
    "        },\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(AUTOINTERP_DIR, \"autointerp_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved showcase.json and autointerp_summary.json to {AUTOINTERP_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}