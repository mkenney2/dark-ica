{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICA Analysis of SAE Residuals (\"Dark Matter\")\n",
    "\n",
    "Decompose the ~35% reconstruction error left over after SAE decomposition of GPT-2 activations using ICA.\n",
    "Determine whether the SAE residual contains interpretable structure that SAEs miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Environment Setup & Imports\n# Uncomment the following line to install dependencies:\n# !pip install sae-lens transformer-lens torch scikit-learn datasets matplotlib scipy numpy\n\nimport os\nimport json\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\nfrom sklearn.decomposition import PCA, FastICA\n\n# Device selection\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\nif DEVICE == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# Create outputs directory\nOUTPUT_DIR = \"outputs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Constants\nTARGET_TOKENS = 1_000_000\nCONTEXT_LEN = 128\nLAYER = 6\nRANDOM_SEED = 42\nHOOK_NAME = f\"blocks.{LAYER}.hook_resid_pre\"\n\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\nprint(f\"\\nConfiguration:\")\nprint(f\"  Target tokens: {TARGET_TOKENS:,}\")\nprint(f\"  Context length: {CONTEXT_LEN}\")\nprint(f\"  Layer: {LAYER}\")\nprint(f\"  Hook: {HOOK_NAME}\")\nprint(f\"  Random seed: {RANDOM_SEED}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Model and SAE\n",
    "from sae_lens import SAE\n",
    "\n",
    "# Load GPT-2 small — try HookedSAETransformer first, fall back to HookedTransformer\n",
    "try:\n",
    "    from sae_lens import HookedSAETransformer\n",
    "    model = HookedSAETransformer.from_pretrained(\"gpt2\", device=DEVICE)\n",
    "    print(\"Loaded model via HookedSAETransformer\")\n",
    "except (ImportError, Exception) as e:\n",
    "    print(f\"HookedSAETransformer not available ({e}), falling back to HookedTransformer\")\n",
    "    from transformer_lens import HookedTransformer\n",
    "    model = HookedTransformer.from_pretrained(\"gpt2\", device=DEVICE)\n",
    "    print(\"Loaded model via HookedTransformer\")\n",
    "\n",
    "# Load SAE for layer 6 residual stream\n",
    "# sae-lens v6+ returns just the SAE object (not a 3-tuple)\n",
    "sae = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.6.hook_resid_pre\",\n",
    "    device=DEVICE,\n",
    ")\n",
    "print(\"\\nSAE loaded successfully\")\n",
    "\n",
    "# Print model and SAE info\n",
    "d_model = model.cfg.d_model\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_layers: {model.cfg.n_layers}\")\n",
    "print(f\"  n_heads: {model.cfg.n_heads}\")\n",
    "\n",
    "print(f\"\\nSAE info:\")\n",
    "print(f\"  W_dec shape: {sae.W_dec.shape}\")\n",
    "print(f\"  n_features: {sae.W_dec.shape[0]}\")\n",
    "print(f\"  d_model: {sae.W_dec.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 2b: RELOAD — Load saved data and recompute derived variables\n# Run this (after Cell 1 + Cell 2) to skip Cells 3-8 and jump to Cell 9+.\n# Requires: outputs/*.npy from a previous run, model and sae from Cell 2.\n\nprint(\"Loading saved arrays from outputs/...\")\nresiduals = np.load(os.path.join(OUTPUT_DIR, \"residuals.npy\"))\nactivations = np.load(os.path.join(OUTPUT_DIR, \"activations.npy\"))\ntokens_flat = np.load(os.path.join(OUTPUT_DIR, \"tokens.npy\"))\nica_directions = np.load(os.path.join(OUTPUT_DIR, \"ica_directions.npy\"))\nica_activations = np.load(os.path.join(OUTPUT_DIR, \"ica_activations.npy\"))\n\nprint(f\"  residuals:       {residuals.shape}  ({residuals.nbytes/1e9:.2f} GB)\")\nprint(f\"  activations:     {activations.shape}  ({activations.nbytes/1e9:.2f} GB)\")\nprint(f\"  tokens_flat:     {tokens_flat.shape}\")\nprint(f\"  ica_directions:  {ica_directions.shape}\")\nprint(f\"  ica_activations: {ica_activations.shape}\")\n\n# --- Recompute Cell 4 variables (variance & kurtosis) ---\ntotal_var = np.var(activations, axis=0).sum()\nresidual_var = np.var(residuals, axis=0).sum()\nfrac_unexplained = residual_var / total_var\nkurt = kurtosis(residuals, axis=0)\nprint(f\"\\nVariance: {frac_unexplained*100:.1f}% unexplained by SAE\")\nprint(f\"Residual kurtosis mean: {kurt.mean():.3f}\")\n\n# --- Recompute Cell 5 variables (PCA component counts) ---\npca_diagnostic = PCA(n_components=min(100, residuals.shape[1]))\npca_diagnostic.fit(residuals)\ncumvar = np.cumsum(pca_diagnostic.explained_variance_ratio_)\nn_components_90 = int(np.searchsorted(cumvar, 0.90) + 1)\nn_components_95 = int(np.searchsorted(cumvar, 0.95) + 1)\nn_components_99 = int(np.searchsorted(cumvar, 0.99) + 1)\nn_ica_components = ica_directions.shape[0]\nprint(f\"PCA: 90%={n_components_90}, 95%={n_components_95}, 99%={n_components_99}\")\nprint(f\"ICA components (from saved): {n_ica_components}\")\n\n# --- Recompute Cell 6 variables (PCA whitening + ICA objects) ---\npca_pre = PCA(n_components=n_ica_components, whiten=True, random_state=RANDOM_SEED)\nresiduals_whitened = pca_pre.fit_transform(residuals)\n\nica = FastICA(\n    n_components=n_ica_components,\n    algorithm='parallel',\n    whiten=False,\n    max_iter=1000,\n    tol=1e-4,\n    random_state=RANDOM_SEED,\n)\nica_sources = ica.fit_transform(residuals_whitened)\ndel residuals_whitened\nprint(f\"ICA refit: converged in {ica.n_iter_} iterations\")\n\n# --- Recompute Cell 7 variables (SAE similarity) ---\nsae_decoder = sae.W_dec.detach().cpu().float().numpy()\nsae_decoder_norms = np.maximum(np.linalg.norm(sae_decoder, axis=1, keepdims=True), 1e-8)\nsae_decoder_normed = sae_decoder / sae_decoder_norms\ncos_sim = ica_directions @ sae_decoder_normed.T\nmax_cos_sim = np.max(np.abs(cos_sim), axis=1)\nbest_sae_match = np.argmax(np.abs(cos_sim), axis=1)\nn_high = int((max_cos_sim > 0.8).sum())\nn_medium = int(((max_cos_sim > 0.3) & (max_cos_sim <= 0.8)).sum())\nn_low = int((max_cos_sim <= 0.3).sum())\ndel sae_decoder, sae_decoder_normed, cos_sim\nprint(f\"SAE similarity: {n_low} novel / {n_medium} medium / {n_high} high\")\n\nd_model = model.cfg.d_model\nprint(f\"\\nReload complete — ready to run Cell 9+\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Collect Activations and Compute Residuals\nfrom datasets import load_dataset\n\n# Stream OpenWebText\ndataset = load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True, trust_remote_code=True)\n\nall_residuals = []\nall_activations = []\nall_tokens = []\ntokens_collected = 0\nbatch_texts = []\nBATCH_SIZE = 32\n\n# Get the BOS/EOT token id so we can exclude it\nBOS_TOKEN_ID = model.tokenizer.bos_token_id  # same as eos for GPT-2: 50256\n\n# Set up tokenizer for proper truncation and padding\nmodel.tokenizer.pad_token = model.tokenizer.eos_token\n\nprint(f\"Collecting ~{TARGET_TOKENS:,} tokens...\")\nprint(f\"Excluding BOS/EOT token (id={BOS_TOKEN_ID}) from all stored data\")\n\nfor i, example in enumerate(dataset):\n    text = example[\"text\"]\n    if len(text.strip()) < 20:\n        continue\n    batch_texts.append(text)\n\n    if len(batch_texts) < BATCH_SIZE:\n        continue\n\n    # Tokenize with explicit truncation to avoid huge intermediate tensors\n    tokenized = model.tokenizer(\n        batch_texts,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        truncation=True,\n        max_length=CONTEXT_LEN - 1,  # leave room for BOS\n    )\n    # Prepend BOS token\n    input_ids = tokenized[\"input_ids\"].to(DEVICE)\n    bos = torch.full((input_ids.shape[0], 1), BOS_TOKEN_ID, dtype=input_ids.dtype, device=DEVICE)\n    tokens = torch.cat([bos, input_ids], dim=1)[:, :CONTEXT_LEN]\n\n    with torch.no_grad():\n        # Run model, cache activations at the hook point\n        _, cache = model.run_with_cache(\n            tokens,\n            names_filter=[HOOK_NAME],\n        )\n\n        # Get activations: shape (batch, seq_len, d_model)\n        acts = cache[HOOK_NAME]\n\n        # Reshape to (batch * seq_len, d_model)\n        acts_flat = acts.reshape(-1, acts.shape[-1])\n        tokens_flat_batch = tokens.reshape(-1)\n\n        # Build mask to exclude BOS/EOT/PAD tokens (all token id 50256 in GPT-2)\n        keep_mask = tokens_flat_batch != BOS_TOKEN_ID\n\n        # Apply mask — only keep real content tokens\n        acts_kept = acts_flat[keep_mask]\n\n        # Run through SAE encode/decode to get reconstruction\n        feature_acts = sae.encode(acts_kept)\n        reconstructed = sae.decode(feature_acts)\n\n        # Compute residual = original - reconstruction\n        residual = acts_kept - reconstructed\n\n        # Store on CPU as numpy\n        all_residuals.append(residual.cpu().float().numpy())\n        all_activations.append(acts_kept.cpu().float().numpy())\n        all_tokens.append(tokens_flat_batch[keep_mask].cpu().numpy())\n\n    # Count tokens kept\n    tokens_collected += int(keep_mask.sum())\n    batch_texts = []\n\n    # Progress report\n    if (i // BATCH_SIZE) % 25 == 0:\n        mem_gb = sum(r.nbytes for r in all_residuals) / 1e9\n        print(f\"  {tokens_collected:,} / {TARGET_TOKENS:,} tokens | {mem_gb:.1f} GB stored\")\n\n    if tokens_collected >= TARGET_TOKENS:\n        break\n\n    # Clear GPU cache periodically\n    if i % 500 == 0 and DEVICE == \"cuda\":\n        torch.cuda.empty_cache()\n\n# Concatenate all collected data\nresiduals = np.concatenate(all_residuals, axis=0)\nactivations = np.concatenate(all_activations, axis=0)\ntokens_flat = np.concatenate(all_tokens, axis=0)\n\n# Free the lists\ndel all_residuals, all_activations, all_tokens\nif DEVICE == \"cuda\":\n    torch.cuda.empty_cache()\n\nprint(f\"\\nCollection complete!\")\nprint(f\"  Tokens kept (excl. BOS/EOT/PAD): {residuals.shape[0]:,}\")\nprint(f\"  Residuals shape: {residuals.shape}\")\nprint(f\"  Activations shape: {activations.shape}\")\nprint(f\"  Tokens shape: {tokens_flat.shape}\")\nprint(f\"  Residuals memory: {residuals.nbytes / 1e9:.2f} GB\")\nprint(f\"  Activations memory: {activations.nbytes / 1e9:.2f} GB\")\n\n# Save intermediates\nnp.save(os.path.join(OUTPUT_DIR, \"residuals.npy\"), residuals)\nnp.save(os.path.join(OUTPUT_DIR, \"activations.npy\"), activations)\nnp.save(os.path.join(OUTPUT_DIR, \"tokens.npy\"), tokens_flat)\nprint(f\"\\nSaved residuals.npy, activations.npy, tokens.npy to {OUTPUT_DIR}/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Basic Diagnostics — Variance & Kurtosis\n",
    "\n",
    "# Fraction of variance unexplained by SAE\n",
    "total_var = np.var(activations, axis=0).sum()\n",
    "residual_var = np.var(residuals, axis=0).sum()\n",
    "frac_unexplained = residual_var / total_var\n",
    "\n",
    "print(f\"Variance Analysis:\")\n",
    "print(f\"  Total activation variance: {total_var:.2f}\")\n",
    "print(f\"  Residual variance: {residual_var:.2f}\")\n",
    "print(f\"  Fraction unexplained by SAE: {frac_unexplained:.3f} ({frac_unexplained*100:.1f}%)\")\n",
    "\n",
    "# Per-dimension kurtosis of residual\n",
    "kurt = kurtosis(residuals, axis=0)  # excess kurtosis (Gaussian = 0)\n",
    "\n",
    "print(f\"\\nResidual Kurtosis (excess):\")\n",
    "print(f\"  Mean: {kurt.mean():.3f}\")\n",
    "print(f\"  Std: {kurt.std():.3f}\")\n",
    "print(f\"  Min: {kurt.min():.3f}\")\n",
    "print(f\"  Max: {kurt.max():.3f}\")\n",
    "print(f\"  Dims with kurtosis > 1: {(kurt > 1).sum()} / {len(kurt)}\")\n",
    "print(f\"  Dims with kurtosis > 3: {(kurt > 3).sum()} / {len(kurt)}\")\n",
    "\n",
    "# Decision point\n",
    "print(f\"\\n--- DECISION POINT ---\")\n",
    "if abs(kurt.mean()) < 0.5:\n",
    "    print(\"Mean kurtosis is near zero — residual is approximately Gaussian.\")\n",
    "    print(\"ICA may not find much structure. This would mean SAEs capture\")\n",
    "    print(\"most non-Gaussian structure, and the dark matter is noise.\")\n",
    "    print(\"(Still a valid and interesting finding!)\")\n",
    "else:\n",
    "    print(f\"Mean kurtosis = {kurt.mean():.3f} — substantial non-Gaussianity detected!\")\n",
    "    print(\"The residual contains non-Gaussian structure that ICA should be able to decompose.\")\n",
    "    print(\"Proceeding with ICA is well-motivated.\")\n",
    "\n",
    "# Plot kurtosis histogram\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(kurt, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Gaussian (kurtosis=0)')\n",
    "ax.axvline(x=kurt.mean(), color='orange', linestyle='-', linewidth=2, label=f'Mean={kurt.mean():.2f}')\n",
    "ax.set_xlabel(\"Excess Kurtosis\", fontsize=12)\n",
    "ax.set_ylabel(\"Count (dimensions)\", fontsize=12)\n",
    "ax.set_title(\"Per-Dimension Kurtosis of SAE Residual\", fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"residual_kurtosis.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved residual_kurtosis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: PCA on Residuals\n",
    "\n",
    "pca_diagnostic = PCA(n_components=min(100, residuals.shape[1]))\n",
    "pca_diagnostic.fit(residuals)\n",
    "\n",
    "cumvar = np.cumsum(pca_diagnostic.explained_variance_ratio_)\n",
    "\n",
    "# Determine optimal component count for 90% variance\n",
    "n_components_90 = int(np.searchsorted(cumvar, 0.90) + 1)\n",
    "n_components_95 = int(np.searchsorted(cumvar, 0.95) + 1)\n",
    "n_components_99 = int(np.searchsorted(cumvar, 0.99) + 1)\n",
    "\n",
    "print(f\"PCA on Residuals:\")\n",
    "print(f\"  Components for 90% variance: {n_components_90}\")\n",
    "print(f\"  Components for 95% variance: {n_components_95}\")\n",
    "print(f\"  Components for 99% variance: {n_components_99}\")\n",
    "print(f\"  Variance in first 10 components: {cumvar[9]*100:.1f}%\")\n",
    "print(f\"  Variance in first 50 components: {cumvar[min(49, len(cumvar)-1)]*100:.1f}%\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(range(1, len(cumvar) + 1), cumvar, 'b-', linewidth=2)\n",
    "ax.axhline(y=0.90, color='r', linestyle='--', alpha=0.7, label=f'90% ({n_components_90} components)')\n",
    "ax.axhline(y=0.95, color='orange', linestyle='--', alpha=0.7, label=f'95% ({n_components_95} components)')\n",
    "ax.axvline(x=n_components_90, color='r', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel(\"Number of PCA Components\", fontsize=12)\n",
    "ax.set_ylabel(\"Cumulative Variance Explained\", fontsize=12)\n",
    "ax.set_title(\"PCA of SAE Residual — Cumulative Variance Explained\", fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0, 1.02])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"residual_pca.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved residual_pca.png\")\n",
    "\n",
    "# Choose n_components for ICA — use 90% variance threshold, but cap at 100\n",
    "n_ica_components = min(n_components_90, 100)\n",
    "print(f\"\\nRecommended ICA component count: {n_ica_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run FastICA\n",
    "\n",
    "print(f\"Running ICA with {n_ica_components} components...\")\n",
    "\n",
    "# PCA pre-whitening\n",
    "pca_pre = PCA(n_components=n_ica_components, whiten=True, random_state=RANDOM_SEED)\n",
    "residuals_whitened = pca_pre.fit_transform(residuals)\n",
    "print(f\"PCA whitening complete. Whitened shape: {residuals_whitened.shape}\")\n",
    "\n",
    "# Run FastICA on whitened data\n",
    "ica = FastICA(\n",
    "    n_components=n_ica_components,\n",
    "    algorithm='parallel',\n",
    "    whiten=False,  # Already whitened via PCA\n",
    "    max_iter=1000,\n",
    "    tol=1e-4,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "ica_sources = ica.fit_transform(residuals_whitened)\n",
    "print(f\"ICA converged in {ica.n_iter_} iterations\")\n",
    "\n",
    "# Compute ICA directions in original 768-dim space\n",
    "# ica.mixing_ shape: (n_ica_components, n_ica_components) — maps sources to whitened space\n",
    "# pca_pre.components_ shape: (n_ica_components, 768) — maps whitened space to original space\n",
    "ica_directions = ica.mixing_.T @ pca_pre.components_  # shape: (n_ica_components, 768)\n",
    "\n",
    "# Normalize each direction to unit length\n",
    "ica_directions = ica_directions / np.linalg.norm(ica_directions, axis=1, keepdims=True)\n",
    "print(f\"ICA directions shape: {ica_directions.shape}\")\n",
    "\n",
    "# --- Robustness check: run with 3 seeds and compare ---\n",
    "print(f\"\\nRunning robustness check with 3 seeds...\")\n",
    "seeds = [42, 123, 999]\n",
    "all_directions = []\n",
    "\n",
    "for seed in seeds:\n",
    "    ica_check = FastICA(\n",
    "        n_components=n_ica_components,\n",
    "        algorithm='parallel',\n",
    "        whiten=False,\n",
    "        max_iter=1000,\n",
    "        tol=1e-4,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    ica_check.fit(residuals_whitened)\n",
    "    dirs = ica_check.mixing_.T @ pca_pre.components_\n",
    "    dirs = dirs / np.linalg.norm(dirs, axis=1, keepdims=True)\n",
    "    all_directions.append(dirs)\n",
    "    print(f\"  Seed {seed}: converged in {ica_check.n_iter_} iterations\")\n",
    "\n",
    "# Compare seeds pairwise: for each component in seed_i, find best match in seed_j\n",
    "print(f\"\\nRobustness (max cosine similarity of best-matched components across seeds):\")\n",
    "for i in range(len(seeds)):\n",
    "    for j in range(i + 1, len(seeds)):\n",
    "        cos_matrix = np.abs(all_directions[i] @ all_directions[j].T)\n",
    "        # For each component in seed_i, find best match in seed_j\n",
    "        best_matches = cos_matrix.max(axis=1)\n",
    "        print(f\"  Seeds {seeds[i]} vs {seeds[j]}: \"\n",
    "              f\"mean best-match cosine = {best_matches.mean():.3f}, \"\n",
    "              f\"min = {best_matches.min():.3f}, \"\n",
    "              f\"components with match > 0.9: {(best_matches > 0.9).sum()}/{n_ica_components}\")\n",
    "\n",
    "del all_directions, ica_check, residuals_whitened\n",
    "\n",
    "# Save ICA directions\n",
    "np.save(os.path.join(OUTPUT_DIR, \"ica_directions.npy\"), ica_directions)\n",
    "print(f\"\\nSaved ica_directions.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compare ICA Directions to SAE Decoder\n",
    "\n",
    "# Extract and normalize SAE decoder weights\n",
    "sae_decoder = sae.W_dec.detach().cpu().float().numpy()  # shape: (n_features, d_model)\n",
    "print(f\"SAE decoder shape: {sae_decoder.shape}\")\n",
    "\n",
    "sae_decoder_norms = np.linalg.norm(sae_decoder, axis=1, keepdims=True)\n",
    "# Avoid division by zero for dead features\n",
    "sae_decoder_norms = np.maximum(sae_decoder_norms, 1e-8)\n",
    "sae_decoder_normed = sae_decoder / sae_decoder_norms\n",
    "\n",
    "# Compute cosine similarity: each ICA direction vs all SAE features\n",
    "# ica_directions shape: (n_ica_components, 768)\n",
    "# sae_decoder_normed shape: (n_features, 768)\n",
    "cos_sim = ica_directions @ sae_decoder_normed.T  # shape: (n_ica_components, n_features)\n",
    "\n",
    "# For each ICA component, find the max absolute cosine similarity with any SAE feature\n",
    "max_cos_sim = np.max(np.abs(cos_sim), axis=1)  # shape: (n_ica_components,)\n",
    "best_sae_match = np.argmax(np.abs(cos_sim), axis=1)\n",
    "\n",
    "# Categorize\n",
    "n_high = int((max_cos_sim > 0.8).sum())\n",
    "n_medium = int(((max_cos_sim > 0.3) & (max_cos_sim <= 0.8)).sum())\n",
    "n_low = int((max_cos_sim <= 0.3).sum())\n",
    "\n",
    "print(f\"\\nICA-to-SAE Similarity:\")\n",
    "print(f\"  Mean max cosine similarity: {max_cos_sim.mean():.3f}\")\n",
    "print(f\"  Median: {np.median(max_cos_sim):.3f}\")\n",
    "print(f\"  High similarity (>0.8):   {n_high:3d} components — SAE already knows these directions\")\n",
    "print(f\"  Medium similarity (0.3-0.8): {n_medium:3d} components — partially overlapping with SAE\")\n",
    "print(f\"  Low similarity (<0.3):    {n_low:3d} components — genuinely NEW directions!\")\n",
    "\n",
    "# Print top-5 most novel and top-5 most similar\n",
    "sorted_by_sim = np.argsort(max_cos_sim)\n",
    "print(f\"\\nMost novel ICA components (lowest SAE similarity):\")\n",
    "for idx in sorted_by_sim[:5]:\n",
    "    print(f\"  Component {idx}: max cosine sim = {max_cos_sim[idx]:.3f} (best SAE match: feature {best_sae_match[idx]})\")\n",
    "\n",
    "print(f\"\\nMost SAE-similar ICA components:\")\n",
    "for idx in sorted_by_sim[-5:][::-1]:\n",
    "    print(f\"  Component {idx}: max cosine sim = {max_cos_sim[idx]:.3f} (best SAE match: feature {best_sae_match[idx]})\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(max_cos_sim, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.axvline(x=0.3, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Low/Medium boundary (0.3)')\n",
    "ax.axvline(x=0.8, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Medium/High boundary (0.8)')\n",
    "ax.set_xlabel(\"Max Cosine Similarity with Any SAE Feature\", fontsize=12)\n",
    "ax.set_ylabel(\"Count (ICA Components)\", fontsize=12)\n",
    "ax.set_title(\"ICA Component Similarity to Nearest SAE Feature\", fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"ica_sae_similarity.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved ica_sae_similarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Interpret ICA Components\n",
    "\n",
    "# Project all activations onto ICA directions to get ICA activations\n",
    "ica_activations = activations @ ica_directions.T  # shape: (N, n_ica_components)\n",
    "print(f\"ICA activations shape: {ica_activations.shape}\")\n",
    "\n",
    "\n",
    "def get_top_activating_examples(component_idx, k=20):\n",
    "    \"\"\"Find the top-k tokens that most strongly activate a given ICA component.\"\"\"\n",
    "    acts = ica_activations[:, component_idx]\n",
    "    top_indices = np.argsort(np.abs(acts))[-k:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        token_id = int(tokens_flat[idx])\n",
    "        token_str = model.to_string([token_id])\n",
    "        # Get surrounding context (+-5 tokens), respecting boundaries\n",
    "        start = max(0, idx - 5)\n",
    "        end = min(len(tokens_flat), idx + 6)\n",
    "        context_ids = tokens_flat[start:end].tolist()\n",
    "        context_str = model.to_string(context_ids)\n",
    "        # Mark the target token position within the context\n",
    "        pos_in_context = idx - start\n",
    "        results.append({\n",
    "            'token': token_str,\n",
    "            'context': context_str,\n",
    "            'activation': float(acts[idx]),\n",
    "            'position': int(idx),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "# Prioritize components with low SAE similarity (most novel)\n",
    "sorted_by_novelty = np.argsort(max_cos_sim)  # ascending — most novel first\n",
    "top_components = sorted_by_novelty[:10]\n",
    "\n",
    "print(f\"\\nInterpreting top 10 most novel ICA components (lowest SAE similarity):\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for comp_idx in top_components:\n",
    "    cos = max_cos_sim[comp_idx]\n",
    "    comp_kurtosis = kurtosis(ica_activations[:, comp_idx])\n",
    "    examples = get_top_activating_examples(comp_idx, k=20)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ICA Component {comp_idx} | Max SAE cosine sim: {cos:.3f} | Kurtosis: {comp_kurtosis:.2f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for ex in examples[:10]:  # Print top 10\n",
    "        act_str = f\"{ex['activation']:+.3f}\"\n",
    "        print(f\"  [{act_str:>8s}]  token='{ex['token']}'\")\n",
    "        print(f\"             context: ...{repr(ex['context'])}...\")\n",
    "\n",
    "# Save ICA activations\n",
    "np.save(os.path.join(OUTPUT_DIR, \"ica_activations.npy\"), ica_activations)\n",
    "print(f\"\\nSaved ica_activations.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Statistical Comparison — ICA vs SAE\n\n# Kurtosis of ICA component activations\nica_kurtosis = kurtosis(ica_activations, axis=0)\nprint(f\"ICA component kurtosis:\")\nprint(f\"  Mean: {ica_kurtosis.mean():.2f}\")\nprint(f\"  Median: {np.median(ica_kurtosis):.2f}\")\nprint(f\"  Min: {ica_kurtosis.min():.2f}, Max: {ica_kurtosis.max():.2f}\")\n\n# Compute kurtosis of active SAE features — STREAMING to avoid OOM\n# We accumulate raw moments (sum of x, x^2, x^3, x^4) per feature across chunks,\n# then compute kurtosis from those without ever storing the full array.\nprint(f\"\\nComputing SAE feature statistics (streaming)...\")\nchunk_size = 50_000\nn_samples = activations.shape[0]\nn_chunks = (n_samples + chunk_size - 1) // chunk_size\n\n# Single pass: accumulate counts, sums, and raw moments for all features\nn_features = sae.W_dec.shape[0]\nfire_counts = np.zeros(n_features, dtype=np.float64)\nsum_x = np.zeros(n_features, dtype=np.float64)\nsum_x2 = np.zeros(n_features, dtype=np.float64)\nsum_x4 = np.zeros(n_features, dtype=np.float64)\n\nfor i in range(n_chunks):\n    start = i * chunk_size\n    end = min(start + chunk_size, n_samples)\n    chunk = torch.tensor(activations[start:end], device=DEVICE, dtype=torch.float32)\n    with torch.no_grad():\n        sae_acts_chunk = sae.encode(chunk).cpu().numpy().astype(np.float64)\n    fire_counts += (sae_acts_chunk > 0).sum(axis=0)\n    sum_x += sae_acts_chunk.sum(axis=0)\n    sum_x2 += (sae_acts_chunk ** 2).sum(axis=0)\n    sum_x4 += (sae_acts_chunk ** 4).sum(axis=0)\n    if (i + 1) % 5 == 0:\n        print(f\"  Chunk {i+1}/{n_chunks} processed\")\n    del sae_acts_chunk\n\nif DEVICE == \"cuda\":\n    torch.cuda.empty_cache()\n\n# Identify active features (fire on >0.1% of tokens)\nfeature_fire_rate = fire_counts / n_samples\nactive_mask = feature_fire_rate > 0.001\nn_active = int(active_mask.sum())\nprint(f\"  Active SAE features (fire >0.1%): {n_active} / {n_features}\")\n\n# Compute excess kurtosis from raw moments for active features\n# kurtosis = E[(X-mu)^4] / Var(X)^2 - 3\n# E[(X-mu)^4] = E[X^4] - 4*mu*E[X^3] + 6*mu^2*E[X^2] - 3*mu^4\n# But we didn't store E[X^3]. Use the simpler form:\n# Var = E[X^2] - E[X]^2\n# E[(X-mu)^4] = E[X^4] - 4*E[X]*E[X^3] + 6*E[X]^2*E[X^2] - 3*E[X]^4\n# Since we skipped x^3, use the relationship:\n# excess_kurtosis = (m4 / m2^2) - 3\n# where m2 = central 2nd moment, m4 = central 4th moment\n# m2 = E[X^2] - mu^2\n# m4 = E[X^4] - 4*mu*E[X^3] + 6*mu^2*E[X^2] - 3*mu^4\n# We need E[X^3], so let's do a second quick pass just for that\nsum_x3 = np.zeros(n_features, dtype=np.float64)\nfor i in range(n_chunks):\n    start = i * chunk_size\n    end = min(start + chunk_size, n_samples)\n    chunk = torch.tensor(activations[start:end], device=DEVICE, dtype=torch.float32)\n    with torch.no_grad():\n        sae_acts_chunk = sae.encode(chunk).cpu().numpy().astype(np.float64)\n    sum_x3 += (sae_acts_chunk ** 3).sum(axis=0)\n    del sae_acts_chunk\n\nif DEVICE == \"cuda\":\n    torch.cuda.empty_cache()\n\n# Now compute kurtosis for active features\nmean = sum_x[active_mask] / n_samples\nex2 = sum_x2[active_mask] / n_samples\nex3 = sum_x3[active_mask] / n_samples\nex4 = sum_x4[active_mask] / n_samples\n\nvar = ex2 - mean**2\n# Central 4th moment: E[(X-mu)^4] = E[X^4] - 4*mu*E[X^3] + 6*mu^2*E[X^2] - 3*mu^4\nm4 = ex4 - 4*mean*ex3 + 6*(mean**2)*ex2 - 3*mean**4\n\n# Avoid division by zero for features with near-zero variance\nvalid = var > 1e-12\nsae_kurtosis = np.full(n_active, np.nan)\nsae_kurtosis[valid] = (m4[valid] / var[valid]**2) - 3.0\n\n# Drop NaN values for stats\nsae_kurtosis_clean = sae_kurtosis[~np.isnan(sae_kurtosis)]\nprint(f\"\\nSAE feature kurtosis (active features, {len(sae_kurtosis_clean)} valid):\")\nprint(f\"  Mean: {sae_kurtosis_clean.mean():.2f}\")\nprint(f\"  Median: {np.median(sae_kurtosis_clean):.2f}\")\nprint(f\"  Min: {sae_kurtosis_clean.min():.2f}, Max: {sae_kurtosis_clean.max():.2f}\")\n\ndel sum_x, sum_x2, sum_x3, sum_x4, fire_counts\n\n# Plot overlapping histograms\nfig, ax = plt.subplots(figsize=(10, 5))\n# Clip extreme values for better visualization\nica_kurt_clipped = np.clip(ica_kurtosis, -10, 200)\nsae_kurt_clipped = np.clip(sae_kurtosis_clean, -10, 200)\nax.hist(ica_kurt_clipped, bins=50, alpha=0.5, label=f'ICA components (n={len(ica_kurtosis)})', density=True, color='steelblue')\nax.hist(sae_kurt_clipped, bins=50, alpha=0.5, label=f'SAE features (n={len(sae_kurtosis_clean)})', density=True, color='orange')\nax.set_xlabel(\"Excess Kurtosis\", fontsize=12)\nax.set_ylabel(\"Density\", fontsize=12)\nax.set_title(\"Kurtosis: ICA Components vs Active SAE Features\", fontsize=14)\nax.legend(fontsize=11)\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"ica_vs_sae_kurtosis.png\"), dpi=150, bbox_inches='tight')\nplt.show()\nprint(f\"Saved ica_vs_sae_kurtosis.png\")\n\n# How much of the residual variance does ICA explain? (sanity check)\nica_reconstruction = ica_sources @ ica.mixing_.T @ pca_pre.components_\nica_recon_var = np.var(residuals - ica_reconstruction, axis=0).sum()\nica_frac_explained = 1.0 - ica_recon_var / residual_var\nprint(f\"\\nICA explains {ica_frac_explained*100:.1f}% of residual variance\")\nprint(f\"(Expected ~{pca_pre.explained_variance_ratio_.sum()*100:.1f}% based on PCA components kept)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Save Results & Summary Report\n\nresults = {\n    \"model\": \"gpt2-small\",\n    \"sae\": \"gpt2-small-res-jb\",\n    \"sae_id\": \"blocks.6.hook_resid_pre\",\n    \"layer\": LAYER,\n    \"tokens_analyzed\": int(residuals.shape[0]),\n    \"d_model\": int(d_model),\n    \"n_ica_components\": int(n_ica_components),\n    \"variance_analysis\": {\n        \"total_activation_variance\": float(total_var),\n        \"residual_variance\": float(residual_var),\n        \"fraction_unexplained_by_sae\": float(frac_unexplained),\n    },\n    \"residual_kurtosis\": {\n        \"mean\": float(kurt.mean()),\n        \"std\": float(kurt.std()),\n        \"min\": float(kurt.min()),\n        \"max\": float(kurt.max()),\n    },\n    \"pca\": {\n        \"n_components_90pct_variance\": int(n_components_90),\n        \"n_components_95pct_variance\": int(n_components_95),\n        \"n_components_99pct_variance\": int(n_components_99),\n    },\n    \"ica\": {\n        \"n_components\": int(n_ica_components),\n        \"iterations_to_converge\": int(ica.n_iter_),\n        \"fraction_residual_variance_explained\": float(ica_frac_explained),\n    },\n    \"ica_sae_similarity\": {\n        \"mean_max_cosine\": float(max_cos_sim.mean()),\n        \"median_max_cosine\": float(np.median(max_cos_sim)),\n        \"high_similarity_count_gt_0.8\": int(n_high),\n        \"medium_similarity_count_0.3_to_0.8\": int(n_medium),\n        \"low_similarity_count_lt_0.3\": int(n_low),\n    },\n    \"kurtosis_comparison\": {\n        \"ica_component_kurtosis_mean\": float(ica_kurtosis.mean()),\n        \"ica_component_kurtosis_median\": float(np.median(ica_kurtosis)),\n        \"sae_feature_kurtosis_mean\": float(sae_kurtosis_clean.mean()),\n        \"sae_feature_kurtosis_median\": float(np.median(sae_kurtosis_clean)),\n    },\n}\n\n# Save JSON\nresults_path = os.path.join(OUTPUT_DIR, \"ica_dark_matter_results.json\")\nwith open(results_path, \"w\") as f:\n    json.dump(results, f, indent=2)\nprint(f\"Saved results to {results_path}\")\n\n# Print formatted summary\nprint(f\"\\n{'='*70}\")\nprint(f\"  ICA DARK MATTER ANALYSIS — SUMMARY REPORT\")\nprint(f\"{'='*70}\")\nprint(f\"\")\nprint(f\"  Model: GPT-2 small (d_model={d_model})\")\nprint(f\"  SAE: gpt2-small-res-jb, layer {LAYER}\")\nprint(f\"  Tokens analyzed: {residuals.shape[0]:,}\")\nprint(f\"\")\nprint(f\"  --- Variance ---\")\nprint(f\"  SAE reconstruction error: {frac_unexplained*100:.1f}% of total variance\")\nprint(f\"  ICA captures {ica_frac_explained*100:.1f}% of that residual variance\")\nprint(f\"\")\nprint(f\"  --- Residual Structure ---\")\nprint(f\"  Mean residual kurtosis: {kurt.mean():.3f}\", end=\"\")\nif abs(kurt.mean()) < 0.5:\n    print(f\" (near-Gaussian — limited non-Gaussian structure)\")\nelse:\n    print(f\" (non-Gaussian — ICA found meaningful structure!)\")\nprint(f\"  PCA components for 90% residual variance: {n_components_90}\")\nprint(f\"\")\nprint(f\"  --- ICA vs SAE ---\")\nprint(f\"  ICA components: {n_ica_components}\")\nprint(f\"  Mean max cosine similarity to SAE: {max_cos_sim.mean():.3f}\")\nprint(f\"  Novel directions (cosine <0.3): {n_low} ({n_low/n_ica_components*100:.0f}%)\")\nprint(f\"  Partially overlapping (0.3-0.8): {n_medium} ({n_medium/n_ica_components*100:.0f}%)\")\nprint(f\"  Redundant with SAE (>0.8):       {n_high} ({n_high/n_ica_components*100:.0f}%)\")\nprint(f\"\")\nprint(f\"  --- Sparsity ---\")\nprint(f\"  ICA component kurtosis (mean): {ica_kurtosis.mean():.2f}\")\nprint(f\"  SAE feature kurtosis (mean):   {sae_kurtosis_clean.mean():.2f}\")\nprint(f\"\")\n\n# Interpretation\nprint(f\"  --- Interpretation ---\")\nif n_low > n_ica_components * 0.3:\n    print(f\"  A substantial fraction ({n_low}/{n_ica_components}) of ICA components represent\")\n    print(f\"  genuinely novel directions not captured by the SAE. The dark matter contains\")\n    print(f\"  interpretable structure that SAEs miss.\")\nelif n_high > n_ica_components * 0.5:\n    print(f\"  Most ICA components ({n_high}/{n_ica_components}) closely match SAE features.\")\n    print(f\"  The dark matter is mostly due to SAE under-reconstruction of known features,\")\n    print(f\"  not genuinely missing features. Better SAE training could reduce this.\")\nelse:\n    print(f\"  The ICA components show a mix of novel and SAE-overlapping directions.\")\n    print(f\"  Some dark matter is structured, some overlaps with known features.\")\nprint(f\"\")\nprint(f\"{'='*70}\")\nprint(f\"\\nOutput files:\")\nfor f_name in [\"residuals.npy\", \"activations.npy\", \"tokens.npy\",\n               \"ica_directions.npy\", \"ica_activations.npy\",\n               \"residual_kurtosis.png\", \"residual_pca.png\",\n               \"ica_sae_similarity.png\", \"ica_vs_sae_kurtosis.png\",\n               \"ica_dark_matter_results.json\"]:\n    full_path = os.path.join(OUTPUT_DIR, f_name)\n    exists = os.path.exists(full_path)\n    size = os.path.getsize(full_path) / 1e6 if exists else 0\n    status = f\"{size:.1f} MB\" if exists else \"MISSING\"\n    print(f\"  {f_name:40s} {status}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}